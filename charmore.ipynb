{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb31944-f9a1-4db2-9981-fac584632729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 27466\n",
      "minimum length: 2\n",
      "maximum length: 16\n"
     ]
    }
   ],
   "source": [
    "### Prepare Dataset\n",
    "import os\n",
    "\n",
    "data_path = 'data/'\n",
    "file_paths = os.listdir(data_path)\n",
    "\n",
    "# List of words\n",
    "S = []\n",
    "\n",
    "def clean_word(word):\n",
    "    return ''.join(filter(str.isalpha, word)).lower().replace(' ', '')\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(data_path + file_path, 'r') as file:\n",
    "        words = [ clean_word(word) for word in file.read().splitlines() ]\n",
    "        S.extend(words)\n",
    "        file.close()\n",
    "\n",
    "S = list(sorted(filter(None, set(S))))\n",
    "\n",
    "print(f'total words: {len(S)}')\n",
    "print(f'minimum length: {min(len(w) for w in S)}')\n",
    "print(f'maximum length: {max(len(w) for w in S)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9ae1c2-3d0b-45a5-9ab7-99f755807bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', '.'), 7377),\n",
       " (('a', 'n'), 4350),\n",
       " (('a', 'r'), 3579),\n",
       " (('n', '.'), 3153),\n",
       " (('r', 'i'), 3059),\n",
       " (('.', 'a'), 3004),\n",
       " (('e', '.'), 2978),\n",
       " (('m', 'a'), 2824),\n",
       " (('n', 'a'), 2816),\n",
       " (('.', 's'), 2806)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Bigrams\n",
    "B = {}\n",
    "for w in S:\n",
    "    chs = list('.' + w + '.')\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        B[bigram] = B.get(bigram, 0) + 1\n",
    "\n",
    "sorted(B.items(), key=lambda kv: -kv[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df2f8016-b4d7-44fe-9904-444835a31a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Simple encoders and decoders\n",
    "chars = sorted(list(set(''.join(S))))\n",
    "stoi = { s: i + 1 for i, s in enumerate(chars) }\n",
    "stoi['.'] = 0\n",
    "itos = { i: s for s, i in stoi.items() }\n",
    "len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a147f12b-6299-468d-94a8-cba3fea2249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create tensor to represent bigrams in a matrix\n",
    "import torch\n",
    "\n",
    "N = torch.zeros((48, 48), dtype=torch.int32)\n",
    "\n",
    "for w in S:\n",
    "    chs = list('.' + w + '.')\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881294d1-cdaf-4998-90e2-727c443b196d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 3004, 1144,  854,  905, 1440,  751,  866, 1289,  729, 1292, 1298,\n",
       "         1178, 2632, 1259,  317,  745,   59, 1378, 2806, 1212,  201,  766,  361,\n",
       "           62,  339,  454,   15,    0,    4,   25,    1,    1,    0,    4,    0,\n",
       "            0,    3,    0,    0,    6,    0,   28,    9,    0,   11,    0,   18],\n",
       "        [7377,  489,  685,  191,  917,  221,  265,  295,  742,  916,  356,  474,\n",
       "         1685, 1357, 4350,   84,  144,   51, 3579, 1222, 1068,  290,  323,  222,\n",
       "           53,  459,  355,    0,    0,    0,    0,    0,    0,    0,    5,    0,\n",
       "            5,    0,    1,    4,    0,    0,    0,    0,    1,    0,    0,    0]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54fcd1d7-88da-433f-bb77-43909683b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare training set\n",
    "X, Y = [], []\n",
    "\n",
    "for w in S:\n",
    "    chs = list('.' + w + '.')\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        X.append(stoi[ch1])\n",
    "        Y.append(stoi[ch2])\n",
    "        \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "n_samples = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05e4dcbe-c9b7-48d8-afb2-705f0d11f444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: 199688\n",
      "targets: 199688\n"
     ]
    }
   ],
   "source": [
    "print(f'inputs: {n_samples}')\n",
    "print(f'targets: {len(Y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ba4db1f-c5d4-4bea-a365-6cad536a1011",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple neural network with only one layer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# prepare weights for the layer\n",
    "G = torch.Generator().manual_seed(909078)\n",
    "W = torch.randn((48, 48), generator=G, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3355f72-7ec1-4935-999b-b02f0f1fc05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training phase\n",
    "E = 200 # epochs\n",
    "L = 50 # learning rate\n",
    "softening_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf6f470a-dbcc-465a-a60b-d895b706b394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 3.1465213298797607\n",
      "epoch: 20, loss: 2.923682689666748\n",
      "epoch: 30, loss: 2.841978073120117\n",
      "epoch: 40, loss: 2.8001396656036377\n",
      "epoch: 50, loss: 2.775259256362915\n",
      "epoch: 60, loss: 2.7589099407196045\n",
      "epoch: 70, loss: 2.7473530769348145\n",
      "epoch: 80, loss: 2.7387678623199463\n",
      "epoch: 90, loss: 2.7321460247039795\n",
      "epoch: 100, loss: 2.726881265640259\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, 100 + 1):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(X, num_classes=48).float()\n",
    "    logits = xenc @ W\n",
    "    # softmax activating\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    # negative log likelihood\n",
    "    loss = -probs[torch.arange(n_samples), Y].log().mean() + softening_rate * (W**2).mean()\n",
    "    if e % 10 == 0:\n",
    "        print(f'epoch: {e}, loss: {loss.item()}')\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    W.data += -L * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "848a03ca-4dd8-40cd-8e0c-18a323c52710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated word: adeva.\n",
      "generated word: la.\n",
      "generated word: terananarif.\n",
      "generated word: arerilard.\n",
      "generated word: kl√∫ez.\n",
      "generated word: ra.\n",
      "generated word: uvinnn.\n",
      "generated word: shieelieyardysa.\n",
      "generated word: if.\n",
      "generated word: maria.\n"
     ]
    }
   ],
   "source": [
    "### Testing the neural net\n",
    "\n",
    "# This seed needs to be changed\n",
    "# if different result is expected\n",
    "seed = 40\n",
    "G = torch.Generator().manual_seed(seed)\n",
    "\n",
    "for _ in range(10):\n",
    "    word = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([i]), num_classes=48).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        prob = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "        i = torch.multinomial(prob, num_samples=1, replacement=True, generator=G).item()\n",
    "        word.append(itos[i])\n",
    "        if i == 0:\n",
    "            break\n",
    "    word = ''.join(word)\n",
    "    print(f'generated word: {word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c17321-811b-4475-8fc2-5c87c959f941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
